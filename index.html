<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /> -->
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Deep Learning-based Auto-Generation of Extensible Timeline</title>
    <style>
        * {
            box-sizing: border-box;
            font-family: "Trebuchet MS", Helvetica, sans-serif;
        }

        a {
            color: inherit;
        }

        p {
            text-align: justify;
        }

        cite {
            font-style: normal;
        }

        body {
            box-sizing: border-box;
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: Verdana, Geneva, sans-serif;
            margin: 0;
            padding: 0;
        }

        .teaser {
            position: relative;
        }

        img,
        video {
            width: 100%;
        }

        .teaser .mask {
            position: absolute;
            top: 0;
            bottom: 0;
            left: 0;
            right: 0;
            background-color: #2c3e50;
            opacity: 0.5;
        }

        .teaser .teaser-content {
            position: absolute;
            bottom: 30%;
            /* left: 50%;
            transform: translate(-50%, -50%); */
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .teaser-content h1 {
            color: #ecf0f1;
            font-size: calc(0.5rem + 2.5vw);
            margin-bottom: 0;
            margin-top: 0;
            letter-spacing: 0.25rem;
            text-align: center;
            width: 80%;
            margin-bottom: 4rem;
        }

        .teaser .jump-btn-group {
            display: flex;
            justify-content: space-around;
            border: 1px #fff solid;
            border-radius: 4px;
        }

        .teaser .jump-btn:hover {
            background-color: rgba(255, 255, 255, 0.075);
        }

        .teaser .jump-btn {
            border-left: 0.5px #fff solid;
            min-width: 6rem;
            height: 2.5rem;
            padding-left: 1rem;
            padding-right: 1rem;
            display: flex;
            justify-content: center;
            align-items: center;
            color: #fff;
            cursor: pointer;
            letter-spacing: 0.2rem;
            font-size: 0.8rem;
            text-transform: uppercase;
            text-decoration: none;
        }

        section {
            margin: 32px auto;
        }

        section h2 {
            text-align: center;
        }

        .abstract {
            text-align: justify;
        }

        .item {
            width: 24%;
            position: relative;
        }

        .item-2 {
            width: 48%;
        }

        .item-3 {
            width: 32%;
        }

        .examples .gallary {
            width: 120%;
            margin-left: -10%;
        }

        .examples .item img {
            /* max-height: 100%;
            max-width: 100%;
            height: auto;
            width: auto; */
            position: relative;
            top: 50%;
            transform: translateY(-50%);

        }

        .examples .gallary .item {
            /* height: 256px; */
            /* text-align: center; */
            border: 1px solid #dddddd;
            border-radius: 5px;
            margin-bottom: 8px;
        }

        table {
            font-family: arial, sans-serif;
            border-collapse: collapse;
            margin: 16px auto;
        }

        td,
        th {
            border: 1px solid #dddddd;
            text-align: left;
            padding: 8px;
        }

        tr:nth-child(even) {
            background-color: #dddddd;
        }

        .gallary {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-between;
            align-content: space-between;
            padding: 8px 8px 0px 8px;
        }

        .gallary:hover {
            border: 1px solid #27ae60;
            border-radius: 10px;
        }

        .paper .gallary .item {
            width: 19%;
            margin-bottom: 8px;
        }

        .paper .gallary img {
            /* padding: 10px 5px 10px 5px; */
            border: 1px solid #dddddd;
            border-radius: 5px;
        }

        .training pre {
            display: flex;
            justify-content: space-between;
        }

        .training pre code {
            width: 48%;
            font-size: 0.8em;
        }

        .list_losses {
            width: 90%;
            margin: auto;
        }

        .list_losses p {
            margin-top: 0;
            margin-bottom: 1.25rem;
            /* border: 1px solid #dddddd;
            border-radius: 5px; */
            padding: 1.25rem;
            background-color: #fff;
            background-clip: border-box;
            border: 1px solid rgba(0, 0, 0, .125);
            border-radius: .25rem;
        }

        .list_losses>span {
            display: block;
            /* border: 1px solid #dddddd;
            border-radius: 5px;
            padding-left: 8px; */
            background: steelblue;
            color: white;

            padding: .5rem 1.25rem;
            margin-bottom: 0;
            border-bottom: 1px solid rgba(0, 0, 0, .125);
            border-radius: calc(.25rem - 1px) calc(.25rem - 1px) 0 0;
        }

        .legend {
            font-size: 1rem;
            position: absolute;
            text-align: center;
            line-height: 30px;
            align-items: center;
            /* margin: 8px; */
            background: #010101;
            color: #fff;
            display: inline-block;
            width: 30px;
            /* height: 16px; */
            border-radius: 50%;
            /* border: 2px solid #f7f7f7; */
            bottom: 0;
            transform: translateX(-50%);
        }
    </style>
    <script src="//code.jquery.com/jquery-3.3.1.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
    <link rel="stylesheet" href="styles/tomorrow-night.css">
    <script src="js/highlight.pack.js"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML' async></script>
</head>

<body>
    <div class="teaser">
        <img src="./figs/bg4_low.jpg" />
        <div class="mask"></div>
        <div class="teaser-content">
            <h1>Towards Automated Infographic Design: <br /> Deep Learning-based Auto-Generation of Extensible Timeline
            </h1>
            <div class="jump-btn-group">
                <!-- <a href="#abstract" class="jump-btn">Abstract</a> -->
                <a href="#labels" class="jump-btn">Labels</a>
                <a href="#architecture" class="jump-btn">Architecture</a>
                <a href="#training" class="jump-btn">Training</a>
                <a href="#paper" class="jump-btn">Paper</a>
                <a href="#examples" class="jump-btn">Examples</a>
                <!-- <a href="#materials" class="jump-btn">Materials</a> -->
            </div>
        </div>
    </div>
    <div class="content" style="width:80vw;max-width: 1024px;">
        <section id="abstract" class="abstract">
            <h2>Abstract</h2>
            Designers need to consider not only perceptual effectiveness but also visual styles when creating an
            infographic.
            This process can be difficult and time consuming for professional designers, not to mention non-expert
            users, leading to the demands of automated infographics design.
            As a first step, we focus on timeline infographics, which have been widely used for centuries.
            We contribute an end-to-end approach that automatically extracts an extensible and extendable timeline
            template from a bitmap image.
            Our approach adopts a deconstruction and reconstruction paradigm. At the deconstruction stage,
            we propose a multi-task deep neural network that simultaneously parses two kinds of information from a
            bitmap timeline:
            1) the global information, which includes the <em>representation</em>, <em>scale</em>, <em>layout</em>, and
            <em>orientation</em> of the timeline,
            and 2) the local information, which includes the location, category, and pixels of each visual element on
            the timeline.
            At the reconstruction stage, we propose a pipeline with three techniques, <em>i.e.</em>, <em>Non-Maximum
                Merging</em>, <em>Redundancy Recover</em>, and <em>DL GrabCut</em>,
            to extract an extensible template from the infographic, by utilizing the deconstruction results. To evaluate
            the effectiveness of our approach,
            we synthesize a timeline dataset (4296 images) and collect a real-world timeline dataset (393 images) from
            the Internet.
            We first report quantitative evaluation results of our approach over the two datasets.
            Then, we present examples of automatically extracted templates and timelines automatically generated based
            on
            these templates to qualitatively demonstrate the performance. The results confirm that our approach can
            effectively extract extensible templates from real-world timeline infographics.
        </section>
        <hr />
        <section id="labels" class="labels">
            <h2>Labels of elements</h2>
            <figure style="text-align: center;
            width: 70%;
            margin: 0 auto 20px auto;">
                <img src="figs/34_labels.png" alt='missing' />
                <figcaption>Categories of elements in a timeline infographic. The <em>event mark</em>, <em>annotation
                        mark</em>,
                    and <em>main body</em> can be reused, while others need to be updated.</figcaption>
            </figure>

            <p>
                We use two datasets to train the model and evaluate our approach.
                The first one (referred to as \(D_1\)) is a synthetic dataset.
                We extended <a href="https://github.com/Microsoft/timelinestoryteller">TimelineStoryteller</a>,
                a timeline authoring tool,
                to generate \(D_1\), covering all types of timeline.
                The second dataset (referred to as \(D_2\)) consists of real-world timelines,
                collected from <a href="https://www.google.com/imghp">Google Image</a>,
                <a href="https://www.pinterest.com">Pinterest</a>, and
                <a href="https://www.freepik.com">FreePicker</a>
                by using the search keywords <em>timeline infographics</em> and <em>infographic timeline</em>.
                \(D_2\) has more diverse styles, especially for marks, and it covers most common types of timeline.
            </p>

            <p>
                To identify the categories of elements in a timeline,
                four of the coauthors independently reviewed all the timelines in our two datasets.
                Each of them iteratively summarized a set of
                mutually exclusive categories that can be used to depict elements in a timeline infographic.
                Gathering the reviews resulted in six categories:
            </p>


            <table id="tb_categories" style="width:90%;">
                <thead>
                    <tr>
                        <th style="width:157px;">Category</th>
                        <th>Explaination</th>
                        <th>Label type</th>
                        <th>Occurrence</th>
                    </tr>
                </thead>
                <tbody></tbody>
            </table>

            <p>
                For the elements that need to be reused, we labeled them with their bboxes and masks, which can be used
                to segment these elements from the original infographic for reusing.
                For those that need to be updated, we only labeled them with their bbox, since the contents of these
                elements need to be changed with updated data.
            </p>

            <p>
                We also identified other guide elements (<em>e.g.</em>, the text elements or marks in axes and legends)
                in our datasets.
                However, these elements only exist in \(D_1\).
                Thus, we decided to exclude them in our study.
            </p>

        </section>
        <hr />
        <section id="architecture" class="architecture">
            <h2>Architecture</h2>
            <figure style="text-align: center;
            width: 60%;
            margin: 0 auto 20px auto;">
                <img src="figs/41_architecture.png" alt='missing' />
                <!-- <figcaption>The complete architecture of our model that can parse both global and local information simultaneously.</figcaption> -->
            </figure>
            The above figure presents an overview of the complete architecture of our model that can parse both global
            and local information simultaneously.
            We further present the details of <a href="#archit_resnext"
                style="color:#5B9BD5;"><strong>ResNeXt-FPN</strong></a>,
            <a href="#archit_class" style="color:#5B9BD5;"><strong>Class Head</strong></a>,
            <a href="#archit_rpn" style="color:#70AD47;"><strong>RPN</strong></a>,
            <a href="#archit_box" style="color:#70AD47;"><strong>Box Head</strong></a>, and
            <a href="#archit_mask" style="color:#70AD47;"><strong>Mask Head</strong></a>, respectively.

            <h4 id="archit_resnext">1. ResNeXt-FPN</h4>
            <a data-fancybox="gallery" href="figs/archit/resnext_fpn.jpg">
                <img style="width: 90%; margin-left: 20%" src="figs/archit/resnext_fpn.jpg" alt='missing' />
            </a>
            <p>
                The figure above shows the configurations of the ResNeXt-FPN, which is used to extract multi-scale image
                features, in our model.
                ResNeXt <cite>ref_resnext</cite> achieves state-of-the-art performances in many computer vision tasks,
                we use ResNeXt to extract the features of a timeline infographic.
                It takes a 3-channel image (<em>i.e.</em>, RGB) as input
                and uses one stem block and four groups of bottleneck block to extract features.
                The multipliers of bottleneck blocks are for ResNeXt-50.
                As for ResNeXt-101, the bottleneck block of the stage \(4\) is repeated \(23\) times rather than \(6\)
                times in ResNeXt-50.
                The four groups of bottleneck block output
                a feature hierarchy with a pyramidal shape
                that consists of feature maps with 256, 512, 1024, 2048 channels, respectively.
            </p>

            <p>
                We then pass the feature maps into Feature Pyramid Network <cite>ref_fpn</cite> (FPN).
                FPN is a top-down architecture and can build semantically strong feature maps at multiple scales using
                the feature maps from ResNeXt.
                FPN makes our model scale-invariant and able to handle images of vastly different resolution.
                It outputs four feature maps with \(256d\) (<em>i.e.</em>, 256 channels).
            </p>

            <p>
                Please note that the input image can be any resolution (<em>i.e.</em>, \(width \times height \)).
                Thus, in the figures, we only annotate the input/output with resolutions in parentheses for those
                requiring fixed size resolutions.
            </p>

            <h4 id="archit_class">2. Class Head</h4>
            <a data-fancybox="gallery" href="figs/archit/class_head.jpg">
                <img style="width: 45%; margin-left: 12.5%;" src="figs/archit/class_head.jpg" alt='missing' />
            </a>
            <p>
                We use Class Head that consists of two sibling fully connected (FC) layers to classify the type and
                orientation of a timeline infographic
                by consuming the feature maps from ResNeXt-FPN.
                One problem here is which feature map should be used given ResNeXt-FPN outputs four \(256d\) feature
                maps.
                Considering that the task is to classify the entire image,
                we choose the last feature map that contains the strongest semantics and the largest scale.
                Another alternative is to use the feature map from ResNeXt (<em>i.e.</em>, the \(2048d\) one).
                We used this feature map in our initial architecture to parse the global information.
                However, after extending our architecture to parse the local extra,
                we found out that using the \(256d\) features from FPN
                can stabilize the training and improve the performance.
                We regard this as an advantage of consistent gradients from the local and global information for the
                back propagation.
            </p>

            <h4 id="archit_rpn">3. RPN</h4>
            <a data-fancybox="gallery" href="figs/archit/rpn.jpg">
                <img style="width: 70%; margin-left: 14%;" src="figs/archit/rpn.jpg" alt='missing' />
            </a>
            <p>
                To parse the local information,
                we first feed the feature maps from the ResNeXt-FPN into a Region Proposal Network <cite>ref_fpn</cite>
                (RPN)
                to propose regions that may contain elements in a timeline image.
                RPN is a fully convolutional network (FCN) that simultaneously predicts element locations (by bbox)
                and objectness probability (<em>i.e.</em>, whether there is an object within the bbox) in an image.
                It takes the four feature maps from ResNeXt-FPN as inputs
                and generates anchors (a set of reference bboxes) of various sizes
                for each feature map (<em>e.g.</em>, \(32^2, 64^2, 128^2, 256^2\)
                <!-- for the \(1\text{st}, 2\text{ed}, 3\text{rd}\), and \(4\text{th}\)  -->
                for the \(1\)st, \(2\)ed, \(3\)rd, and \(4\)th
                feature map, respectively).
                For each grid in each feature map, RPN uses an anchor generator to generate three anchors of three
                aspect ratios (<em>i.e.</em>, \(1:2, 1:1, 2:1\)).
                For each three anchors center at the same grid,
                RPN outputs a \(3d\) vector to predict their objectness probability
                and a \(3 \times 4d\) vector to predict their regression offsets.
                A region proposal creator will then process these outputs together with anchors
                to remove bboxes without elements and crop bboxes exceeding the boundary of the image.
                The remaining bboxes are then be used to extract regions of interest (RoIs) from the feature maps using
                a RoIAlign layer.
                Besides, the RoIAlign layer normalize each RoI to fixed sizes
                for passing it to two heads (\(7 \times 7\) for Box Head and \(14 \times 14\) for Mask Head).
            </p>

            <h4 id="archit_box">4. Box Head</h4>
            <a data-fancybox="gallery" href="figs/archit/box_head.jpg">
                <img style="width: 45%; margin-left: 13.5%;" src="figs/archit/box_head.jpg" alt='missing' />
            </a>
            <p>
                The Box Head follows the design in <cite>ref_fast_rcnn</cite>
                to use two sibling FC layers to classify the category
                and regress the bbox of the element within a RoI.
                It takes a \(256d\) feature of resolution \(7 \times 7 \) from RPN as the input
                and two FC layers to reduce the feature to \(1024d\).
                It then uses two sibling FC layers to output: 1) a \(7d\) vector that can be used to compute the
                category over 6 element categories and 1 "catch all" background,
                and 2) a \(6 \times 4d\) vector that represents 6 bbox regressions,
                each of which is a four-value tuple \(t = (t_x, t_y, t_w, t_h)\) for a category.
                We use the parameterization for \(t\) given in <cite>ref_fast_rcnn</cite>,
                in which \(t\)
                specifies a
                scale-invariant translation and log-space height/width shift
                relative to an RoI.
            </p>

            <h4 id="archit_mask">5. Mask Head</h4>
            <a data-fancybox="gallery" href="figs/archit/mask_head.jpg">
                <img style="width: 40%; margin-left: 16%;" src="figs/archit/mask_head.jpg" alt='missing' />
            </a>
            <p>
                The Mask Head follows the design in <cite>ref_mask_rcnn</cite>
                to use an FCN for predicting the pixels of the element within a RoI.
                Specifically,
                it takes a \(256d\) feature of resolution \(14 \times 14 \) from RPN as the input
                and uses 4 Conv2D layers of \(3 \times 3\) kernels, 1 transposed Conv2D layer,
                and 1 Conv2D of \(1 \times 1\) kernel to output 6 binary masks of resolution \(28 \times 28\), one for
                each of the 6 categories.
                The binary masks indicate whether a pixel inside the RoI belongs to the element or not.
            </p>

        </section>
        <hr />
        <section id="training" class="training">
            <h2>Training</h2>
            <h4 id="train_loss">1. Loss Functions</h4>
            <p>
                Our model is optimized for a multi-task loss function that consists of seven losses:
                \begin{equation}
                \begin{split}
                \mathcal{L} &= \lambda_1 \mathcal{L}_{{Image}_{type}} + \lambda_2 \mathcal{L}_{{Image}_{orientation}} \\
                & + \lambda_3 \mathcal{L}_{{RoI}_{objectness}} + \lambda_4 \mathcal{L}_{{RoI}_{bbox}} \\
                & + \lambda_5 \mathcal{L}_{{DT}_{type}} + \lambda_6 \mathcal{L}_{{DT}_{bbox}} + \lambda_7
                \mathcal{L}_{{DT}_{mask}}
                \end{split}
                \end{equation}
            </p>

            <table id="tb_losses" style="width:50%;">
                <thead>
                    <tr>
                        <th></th>
                        <th>Target</th>
                        <th>Type</th>
                        <th>Loss</th>
                        <th>Weight</th>
                    </tr>
                </thead>
                <tbody></tbody>
            </table>

            <p>The summary of these losses is presented in the above table.
                The hyper-parameters \(\lambda\) control the balance between these seven task losses.
                We note that the losses defined on the entire image (<em>i.e.</em>, \(\mathcal{L}_{{Image}_{type}}\) and
                \(\mathcal{L}_{{Image}_{orientation}}\))
                are not on the same scale with other losses (which are defined on the local regions of the image).
                Therefore, we empirically set a smaller \(\lambda\) to them (<em>i.e.</em>, 0.15)
                and follow previous works <cite>ref_fast_rcnn</cite><cite>ref_rpn</cite><cite>ref_mask_rcnn</cite> to
                keep other losses as 1.
                The detail computation of each loss is described as follows:
            </p>

            <div class="list_losses">
                <span>\(\mathcal{L}_{{Image}_{type}}\)</span>
                <p>
                    The \(\mathcal{L}_{{Image}_{type}}\), defined on the entire image,
                    is computed using the output on the timeline type from <a href="#archit_class"
                        style="color:#5B9BD5;"><strong>Class Head</strong></a>.
                    The output is a discrete probability distribution \(p = (p_1, ..., p_{10})\) over 10 timeline types
                    computed by a softmax function.
                    The timeline type classification loss is a log loss for the true type \(u:
                    \mathcal{L}_{{Image}_{type}}(p, u) = -\log{p}_{u}\).
                </p>

                <span>\(\mathcal{L}_{{Image}_{orientation}}\)</span>
                <p>
                    The \(\mathcal{L}_{{Image}_{orientation}}\), defined on the entire image,
                    is computed using the output on the timeline orientation from <a href="#archit_class"
                        style="color:#5B9BD5;"><strong>Class Head</strong></a>.
                    The output is a discrete probability distribution \(p = (p_1, p_2, p_3)\) over 3 timeline
                    orientations computed by a softmax function.
                    The timeline orientation classification loss is a log loss for the true orientation \(u:
                    \mathcal{L}_{{Image}_{orientation}}(p, u) = -\log{p}_{u}\).
                </p>

                <span>\(\mathcal{L}_{{RoI}_{objectness}}\)</span>
                <p>
                    The \(\mathcal{L}_{{RoI}_{objectness}}\), defined on each RoI,
                    is computed using the output on the objectness from <a href="#archit_rpn"
                        style="color:#70AD47;"><strong>RPN</strong></a>.
                    For each RoI, <a href="#archit_rpn" style="color:#70AD47;"><strong>RPN</strong></a> uses a softmax
                    function to compute a
                    probability \(p\) to predict whether the RoI contains objects or not (<em>i.e.</em>, foreground
                    <em>vs.</em> background).
                    The ground truth \(p^*\) is 1 if a RoI is foreground, and is 0 if it is background.
                    The objectness classification loss is a log loss over two classes:
                    \(\mathcal{L}_{{RoI}_{objectness}}(p, p^*) = - p^* \log p - (1 - p^*) \log (1 - p)\).
                    We refer the reader to <cite>ref_rpn</cite> for more details.
                </p>

                <span>\(\mathcal{L}_{{RoI}_{bbox}}\)</span>
                <p>
                    The \(\mathcal{L}_{{RoI}_{bbox}}\), defined on each RoI,
                    is computed using the output on the bbox from <a href="#archit_rpn"
                        style="color:#70AD47;"><strong>RPN</strong></a>.
                    For each RoI, <a href="#archit_rpn" style="color:#70AD47;"><strong>RPN</strong></a> outputs bbox
                    correction \(t = (t_x, t_y, t_w, t_h)\)
                    of the anchor associated with the RoI. The regression loss is computed using Smooth \(L_1\) on the
                    prediction \(t\) and ground truth \(t^* \):
                    \begin{equation}
                    \mathcal{L}_{{RoI}_{bbox}}(t, t^*) = p^* L_1^\text{smooth}(t - t^*),
                    \end{equation}
                    where \( L_1^\text{smooth}(x) = \begin{cases}0.5 x^2 & \text{if} \vert x \vert < 1 \\ \vert x \vert
                        - 0.5 & \text{otherwise} \end{cases}\), and the term \(p^*\) indicates that the loss is
                        activated only for foreground RoI (\(p^*=1\)) and is disabled otherwise (\(p^*=0\)). We refer
                        the reader to <cite>ref_rpn</cite> for more details.
                </p>
                </li>

                <span>\(\mathcal{L}_{{DT}_{type}}\)</span>
                <p>
                    The \(\mathcal{L}_{{DT}_{type}}\), defined on each detection (<em>i.e.</em>, DT),
                    is computed using the output on the element category from <a href="#archit_box"
                        style="color:#70AD47;"><strong>Box Head</strong></a>.
                    For each DT, <a href="#archit_box" style="color:#70AD47;"><strong>Box Head</strong></a> uses a
                    softmax function to compute a
                    discrete probability distribution \(p = (p_0, p_1, ..., p_6)\) over six pre-defined element
                    categories and a "catch all" background.
                    The element category classification loss is a log loss for the true category \(u:
                    \mathcal{L}_{{DT}_{type}}(p, u) = -\log{p}_{u}\).
                </p>

                <span>\(\mathcal{L}_{{DT}_{bbox}}\)</span>
                <p>
                    The \(\mathcal{L}_{{DT}_{bbox}}\), defined on each DT,
                    is computed using the output on the element bbox from <a href="#archit_box"
                        style="color:#70AD47;"><strong>Box Head</strong></a>.
                    For each DT,
                    <a href="#archit_box" style="color:#70AD47;"><strong>Box Head</strong></a> outputs 6 bbox regression
                    corrections, \(t^k = (t^k_x, t^k_y, t^k_w, t^k_h)\) indexed by \(k\),
                    one for each of the 6 categories.
                    We use the parameterization for \(t^k\) given in <cite>ref_fast_rcnn</cite>,
                    in which \(t^k\) specifies a
                    scale-invariant translation and log-space height/width shift relative to a region proposal (RoI).
                    Similar to \(\mathcal{L}_{{RoI}_{bbox}}\), the regression loss \(\mathcal{L}_{{DT}_{bbox}}\)
                    is also computed using Smooth \(L_1\): \(\mathcal{L}_{{RoI}_{bbox}}(t^u, t^*) = [u > 0]
                    L_1^\text{smooth}(t^u - t^*) \),
                    where \(t^u\) is the predicted bbox correction of the true category \(u\) and \(t^*\) is the ground
                    truth.
                    The Iverson bracket indicator function
                    \([u > 0]\) evaluates to 1 when \(u > 0\) and 0 otherwise,
                    which means the loss is only activated on the foreground predictions (\(p_1\) to \(p_6\)), since the
                    "catch all" background class is labeled \(u = 0\) by convention.
                    We refer the reader to <cite>ref_fast_rcnn</cite> for more details.
                </p>

                <span>\(\mathcal{L}_{{DT}_{mask}}\)</span>
                <p>
                    The \(\mathcal{L}_{{DT}_{mask}}\), defined on each DT,
                    is computed using the output of <a href="#archit_mask" style="color:#70AD47;"><strong>Mask
                            Head</strong></a>.
                    For each DT, <a href="#archit_mask" style="color:#70AD47;"><strong>Mask Head</strong></a> outputs 6
                    binary masks of resolution \(m \times m \) (defined as a hyper parameter),
                    one for each of the 6 categories. \(\mathcal{L}_{{DT}_{mask}}\) is defined as the average binary
                    cross-entropy loss over all pixels of a mask.
                    Besides, for an DT associated with its ground true category \(u\), the loss is only defined in the
                    \(u\)-th mask (other mask outputs do not contribute to the loss):
                    \( \mathcal{L}_{{DT}_{mask}} = - [u > 0] \frac{1}{m^2} \sum_{1 \leq i, j \leq m} \big[ p^*_{ij} \log
                    p^u_{ij} + (1-p^*_{ij}) \log (1- p^u_{ij}) \big] \),
                    where \(p^*_{ij}\) is the label of a pixel \((i, j)\) in the true mask
                    and \(p^u_{ij}\) is the predicted label of the same pixel for the true category \(u\); the term \([u
                    > 0]\) works in the same manner as in \(\mathcal{L}_{{DT}_{bbox}}\).
                    We refer the reader to <cite>ref_mask_rcnn</cite> for more details.
                </p>
            </div>

            <h4 id="train_hyper">2. Hyper parameters</h4>
            <pre>
<code class="yaml">MODEL:
    META_ARCHITECTURE: "GeneralizedRCNN"
    WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
    BACKBONE:
        CONV_BODY: "R-101-FPN"
        OUT_CHANNELS: 256
    CLASSIFIER:
        NUM_CLASSES: 10 
    CLASSIFIER2:
        NUM_CLASSES: 3
    RPN:
        USE_FPN: True
        ANCHOR_STRIDE: (4, 8, 16, 32, 64)
        PRE_NMS_TOP_N_TRAIN: 2000
        PRE_NMS_TOP_N_TEST: 1000
        POST_NMS_TOP_N_TEST: 1000
        FPN_POST_NMS_TOP_N_TEST: 1000
    ROI_HEADS:
        USE_FPN: True
        BATCH_SIZE_PER_IMAGE: 256
    ROI_BOX_HEAD:
        POOLER_RESOLUTION: 7
        POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
        POOLER_SAMPLING_RATIO: 2
        FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
        PREDICTOR: "FPNPredictor"
        NUM_CLASSES: 7
    ROI_MASK_HEAD:
        POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
        FEATURE_EXTRACTOR: "MaskRCNNFPNFeatureExtractor"
        PREDICTOR: "MaskRCNNC4Predictor"
        POOLER_RESOLUTION: 14
        POOLER_SAMPLING_RATIO: 2
        EXCLUDE_LABELS: (0, 3)
        RESOLUTION: 28
        SHARE_BOX_FEATURE_EXTRACTOR: False
    RESNETS:
        STRIDE_IN_1X1: False
        NUM_GROUPS: 32
        WIDTH_PER_GROUP: 8
    MASK_ON: True
    CLASSIFIER_ON: True 
    CLASSIFIER2_ON: True
INPUT:
    MIN_SIZE_TRAIN: 833
    MAX_SIZE_TRAIN: 1024
    MIN_SIZE_TEST: 833 
    MAX_SIZE_TEST: 1024
DATALOADER:
    SIZE_DIVISIBILITY: 32
    ASPECT_RATIO_GROUPING: False
SOLVER:
    BASE_LR: 0.005
    WEIGHT_DECAY: 0.0001
    STEPS: (56000, 76000)
    # Epoch = (MAX_ITER * IMS_PER_BATCH) / #dataset
    MAX_ITER: 84000
    IMS_PER_BATCH: 4
    CHECKPOINT_PERIOD: 10000</code>
<code class="yaml">MODEL:
    META_ARCHITECTURE: "GeneralizedRCNN"
    WEIGHT: "catalog://ImageNetPretrained/MSRA/R-50"
    BACKBONE:
        CONV_BODY: "R-50-FPN"
        OUT_CHANNELS: 256
    CLASSIFIER:
        NUM_CLASSES: 10 
    CLASSIFIER2:
        NUM_CLASSES: 3
    RPN:
        USE_FPN: True
        ANCHOR_STRIDE: (4, 8, 16, 32, 64)
        PRE_NMS_TOP_N_TRAIN: 2000
        PRE_NMS_TOP_N_TEST: 1000
        POST_NMS_TOP_N_TEST: 1000
        FPN_POST_NMS_TOP_N_TEST: 1000
    ROI_HEADS:
        USE_FPN: True
        BATCH_SIZE_PER_IMAGE: 256
    ROI_BOX_HEAD:
        POOLER_RESOLUTION: 7
        POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
        POOLER_SAMPLING_RATIO: 2
        FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
        PREDICTOR: "FPNPredictor"
        NUM_CLASSES: 7
    ROI_MASK_HEAD:
        POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
        FEATURE_EXTRACTOR: "MaskRCNNFPNFeatureExtractor"
        PREDICTOR: "MaskRCNNC4Predictor"
        POOLER_RESOLUTION: 14
        POOLER_SAMPLING_RATIO: 2
        RESOLUTION: 28
        SHARE_BOX_FEATURE_EXTRACTOR: False
    MASK_ON: True
    CLASSIFIER_ON: True 
    CLASSIFIER2_ON: True 
INPUT:
    MIN_SIZE_TRAIN: 833
    MAX_SIZE_TRAIN: 1024
    MIN_SIZE_TEST: 833
    MAX_SIZE_TEST: 1024
DATALOADER:
    SIZE_DIVISIBILITY: 32
    ASPECT_RATIO_GROUPING: False
SOLVER:
    BASE_LR: 0.005
    WEIGHT_DECAY: 0.0001
    STEPS: (56000, 76000)
    # Epoch = (MAX_ITER * IMS_PER_BATCH) / #dataset
    MAX_ITER: 84000
    IMS_PER_BATCH: 4 
    CHECKPOINT_PERIOD: 10000</code>
</pre>
        </section>
        <hr />
        <section id="paper" class="paper">
            <h2>Paper</h2>
            <div class="gallary"></div>
        </section>
        <hr />
        <section id="examples" class="examples">
            <h2>Examples</h2>

            <h4>1. Examples outputted from the model and refined by <em>DL GrabCut</em>.</h4>
            Please note that the aliasing of some borders of masks is caused by the rendering method we used
            (<em>i.e.</em>, the <em>findCountor</em> in openCV.)
            <div id="examples_good" class="gallary"></div>

            <h4>2. Supplemental examples</h4>
            <div id="examples_hybrid" class="gallary">
                <div class="item">
                    <a data-fancybox="gallery" href="figs/examples/cat_1.jpg">
                        <img src="figs/examples/cat_1_tn.jpg" alt='missing' /> </a>
                    <span class="legend">a</span>
                </div>
                <div class="item">
                    <a data-fancybox="gallery" href="figs/examples/cat_2.jpg">
                        <img src="figs/examples/cat_2_tn.jpg" alt='missing' /> </a>
                    <span class="legend">b</span>
                </div>
                <div class="item item-2">
                    <a data-fancybox="gallery" href="figs/examples/cat_3.jpg">
                        <img src="figs/examples/cat_3_tn.jpg" alt='missing' /> </a>
                    <span class="legend">c</span>
                </div>
            </div>
            <p>
                Description to be added: hybrid elements.
            </p>

            <div id="examples_scale" class="gallary">
                <div class="item">
                    <a data-fancybox="gallery" href="figs/examples/scale_o.jpg">
                        <img src="figs/examples/scale_o_tn.jpg" alt='missing' /> </a>
                        <span class="legend">a</span>
                </div>
                <div class="item">
                    <a data-fancybox="gallery" href="figs/examples/scale_u.jpg">
                        <img src="figs/examples/scale_u_tn.jpg" alt='missing' /> </a>
                        <span class="legend">b</span>

                </div>
                <div class="item item-2">
                        <a data-fancybox="gallery" href="figs/examples/scale_d.jpg">
                            <img src="figs/examples/scale_d_tn.jpg" alt='missing' /> </a>
                            <span class="legend">c</span>
    
                    </div>
            </div>
            <p>
                Description to be added: multi-sacle test/train.
            </p>
    

            <div id="examples_bw" class="gallary" style="justify-content: space-around;">
                <div class="item item-2">
                    <a data-fancybox="gallery" href="figs/examples/bw_u_o.jpg">
                        <img src="figs/examples/bw_u_o_tn.jpg" alt='missing' /> </a>
                        <span class="legend">a</span>
                </div>
                <div class="item item-2">
                    <a data-fancybox="gallery" href="figs/examples/bw_u.jpg">
                        <img src="figs/examples/bw_u_tn.jpg" alt='missing' /> </a>
                        <span class="legend">b</span>
                </div>

                <div class="item item-2">
                    <a data-fancybox="gallery" href="figs/examples/bw_d_o.jpg">
                        <img src="figs/examples/bw_d_o_tn.jpg" alt='missing' /> </a>
                        <span class="legend">c</span>
                </div>
                <div class="item item-2">
                    <a data-fancybox="gallery" href="figs/examples/bw_d.jpg">
                        <img src="figs/examples/bw_d_tn.jpg" alt='missing' /> </a>
                        <span class="legend">d</span>
                </div>
            </div>
            <p>
                Description to be added: single/multi-channels test/train.
            </p>

        </section>

        <!-- <hr />
        <section id="materials" class="materials">
                <h2>Materials</h2>
                
            <h4>1. Code</h4>
            To be added soon.

            <h4>2. Datasets</h4>
            To be added soon.            
        </section> -->

        <hr />
        <section id="references" class="references">
            <h2 style="text-align:left;">Reference</h2>
            <p id="ref_resnext">Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He. <a
                    href="https://arxiv.org/abs/1611.05431">"Aggregated Residual Transformations for Deep Neural
                    Networks."</a> In Proc. IEEE CVPR. 2017.</p>
            <p id="ref_fpn">Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie. <a
                    href="https://arxiv.org/abs/1612.03144">"Feature Pyramid Networks for Object Detection."</a> In
                Proc. IEEE CVPR. 2017.</p>
            <p id="ref_fast_rcnn">Ross Girshick. <a href="https://arxiv.org/abs/1504.08083">“Fast R-CNN.”</a> In Proc.
                IEEE ICCV. 2015.</p>
            <p id="ref_rpn">Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. <a
                    href="https://arxiv.org/abs/1506.01497">“Faster R-CNN: Towards Real-time Object Detection with
                    Region Proposal Networks.”</a> In Proc. NIPS. 2015.</p>
            <p id="ref_mask_rcnn">Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. <a
                    href="https://arxiv.org/abs/1703.06870">"Mask R-CNN."</a> In Proc. IEEE ICCV. 2017.</p>
        </section>
    </div>

    <script>
        function insertExampleImage() {
            const section = document.querySelector('#examples > #examples_good')
            for (let i = 1; i < 13; ++i) {
                // section.insertAdjacentHTML('beforeend', `
                //     <div class="pair item">
                //         <figure>
                //             <img src="figs/examples/${i}_u.png" alt='missing' />
                //             <figcaption>Before DL GrabCut in RGB</figcaption>
                //         </figure>
                //         <figure>
                //             <img src="figs/examples/${i}_d.png" alt='missing' />
                //             <figcaption>After DL GrabCut in GrayScale</figcaption>
                //         </figure>
                //     </div>
                // `)
                section.insertAdjacentHTML('beforeend', `
                <div class="item">
                        
                    <a data-fancybox="gallery" href="figs/examples/${i}_u.png">
                    <img src="figs/examples/${i}_u_tn.jpg" alt='missing' /> </a>
                </div>
                <div class="item">
                        <a data-fancybox="gallery" href="figs/examples/${i}_d.png">
                    <img src="figs/examples/${i}_d_tn.jpg" alt='missing' />
                    </a>
                </div>
                `)
            }
        }

        function insertCategoryTable() {
            const target = document.querySelector('#tb_categories > tbody')
            const tableData = [
                ['<img style="width:15px;" src="https://placehold.it/15/FFC000/000000?text=+" /> Event mark', 'A graphical mark that represents an event. The mark does not relate to the content of the event it represents.', 'BBox + Mask', '1 / event'],
                ['<img style="width:15px;" src="https://placehold.it/15/34495E/000000?text=+" /> Event text', 'A block of text that depicts and only depicts the occurred time of an event.', 'BBox', '0 ~ 1 / event'],
                ['<img style="width:15px;" src="https://placehold.it/15/5B9BD5/000000?text=+" /> Annotation mark', 'A graphical mark that annotates an event. The mark does not relate to the content of the event it annotates.', 'BBox + Mask', '0 ~ n / event'],
                ['<img style="width:15px;" src="https://placehold.it/15/AF7AC4/000000?text=+" /> Annotation text', 'A block of text that depicts the content of an event. The occurred time of the event can be included.', 'BBox', '0 ~ n / event'],
                ['<img style="width:15px;" src="https://placehold.it/15/FC6868/000000?text=+" /> Annotation icon', 'A graphical or natural image that annotates an event.', 'BBox', '0 ~ n / event'],
                ['<img style="width:15px;" src="https://placehold.it/15/1ABC9C/000000?text=+" /> Main body', 'A graphical mark that represents the time.', 'BBox + Mask', '0 ~ n / image'],
            ]
            for (const line of tableData) {
                target.insertAdjacentHTML('beforeend', `<tr>${line.map(d => `<td>${d}</th>`).join('')}</td>`)
            }
        }

        function insertLossesTable() {
            const target = document.querySelector('#tb_losses > tbody')
            const data = [
                [String.raw`\(\mathcal{L}_{{Image}_{type}}\)`, 'Image', 'Classification', 'Cross-Entropy', 0.15],
                [String.raw`\(\mathcal{L}_{{Image}_{orientation}}\)`, 'Image', 'Classification', 'Cross-Entropy', 0.15],
                [String.raw`\(\mathcal{L}_{{RoI}_{objectness}}\)`, 'RoI', 'Classification', 'Cross-Entropy', 1],
                [String.raw`\(\mathcal{L}_{{RoI}_{bbox}}\)`, 'RoI', 'Regression', String.raw`Smooth \(L_1\)`, 1],
                [String.raw`\(\mathcal{L}_{{DT}_{type}}\)`, 'DT', 'Classification', 'Cross-Entropy', 1],
                [String.raw`\(\mathcal{L}_{{DT}_{bbox}}\)`, 'DT', 'Regression', String.raw`Smooth \(L_1\)`, 1],
                [String.raw`\(\mathcal{L}_{{DT}_{mask}}\)`, 'DT', 'Classification', 'Cross-Entropy', 1]
            ]
            for (const line of data) {
                target.insertAdjacentHTML('beforeend', `<tr>${line.map(d => `<td>${d}</th>`).join('')}</td>`)
            }
        }

        function insertPaperPreview() {
            const target = document.querySelector('#paper > .gallary')
            for (let i = 1; i < 11; ++i) {
                target.insertAdjacentHTML('beforeend', `<a href="./eg7.pdf" class="item"><img src="figs/paper/${i}.jpg"/></a>`)
            }
        }

        function convertReference() {
            const cites = document.querySelectorAll('cite')
            let id = 1
            const orderBook = {}
            cites.forEach(c => {
                if (!(c.textContent in orderBook)) {
                    orderBook[c.textContent] = id++
                }
            })
            cites.forEach(c => {
                const refId = c.textContent
                c.textContent = ''
                c.insertAdjacentHTML('beforeend', `<a href="#${refId}">[${orderBook[refId]}]</a>`)
            })

            // reorder
            const ps = Array.from(document.querySelectorAll('#references > p'))
            ps.forEach(p => {
                p.innerHTML = `[${orderBook[p.id]}] ${p.innerHTML}`
                p.parentElement.removeChild(p)
            })
            const parent = document.querySelector('#references')
            ps.filter(p => orderBook[p.id])
                .sort((a, b) => orderBook[a.id] - orderBook[b.id])
                .forEach(p => parent.appendChild(p))
        }

        // insert categorytable
        insertCategoryTable()

        // insert examples
        insertExampleImage()

        // insert losses
        insertLossesTable()

        // insert paper
        insertPaperPreview()

        // highlight code
        hljs.initHighlightingOnLoad()

        // reference
        convertReference()

    </script>
</body>

</html>